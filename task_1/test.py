# -*- coding: utf-8 -*-
"""CNNModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRhl6GnWdLuhDojQoBZG_X_dK8WDl1Z4
"""

import shutil
import numpy as np
import torch
import torchvision
from torchsummary import summary
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.utils.data as data_utils
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()



# Define transforms for the data
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

dtd_path = '.data\dtd\dtd\images'
checkpoint_path = 'checkpointsoverfit\model.pt'
best_model_path = 'checkpointsoverfit\savebestmodel.pt'

valid_loss_min = np.Inf

# Load the DTD dataset
train_dataset = torchvision.datasets.DTD(root='.data',split='train',download=True, transform=train_transforms)
test_dataset = torchvision.datasets.DTD(root='.data', split='val', download=True, transform=test_transforms)


dtd_dataset = torchvision.datasets.ImageFolder(root=dtd_path, transform=train_transforms)

# train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)
# testset = torchvision.datasets.DTD(root='.data',split='test',download=True, transform=test_transforms)
# test_loader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)

# Use random_split to split the dataset into train and test sets

train_size = int(0.8 * len(dtd_dataset))
val_size = len(dtd_dataset) - train_size
train_dataset, test_dataset = data_utils.random_split(dtd_dataset, [train_size, val_size])

# train_dataset = datasets.DTD(root="data",split='train',download=True, transform=transform)
# test_dataset = datasets.DTD(root="data",split = 'test',download = True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)


class myNet(nn.Module):
    def __init__(self, num_classes=47):
        super(myNet, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            self._make_layer(32, 64, stride=1),
            self._make_layer(64, 128, stride=2),
            self._make_layer(128, 128, stride=1),
            self._make_layer(128, 256, stride=2),
            
            self._make_layer(256, 256, stride=1),
            self._make_layer(256, 512, stride=2),
            
            nn.AdaptiveAvgPool2d(1)
           
        )
        self.dropout = nn.Dropout(p=0.2, inplace=True)
        self.fc = nn.Linear(512, num_classes)
        self._initialize_weights()

    # def _make_layer(self, in_channels, out_channels, stride,groups):
    #     layers = []
    #     layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1,groups=groups, bias=False))
    #     layers.append(nn.BatchNorm2d(out_channels))
    #     layers.append(nn.ReLU(inplace=True))

    #     # layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))
    #     # layers.append(nn.BatchNorm2d(out_channels))
    #     # layers.append(nn.ReLU(inplace=True))
    #     # return nn.Sequential(*layers)
    def _make_layer(self, in_channels, out_channels, stride):
        layers = []

        layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False))
        layers.append(nn.BatchNorm2d(in_channels))
        layers.append(nn.ReLU(inplace=True))

        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        return nn.Sequential(*layers)
    
    def _make_dropout_layer(self):
        return nn.Dropout(p=0.2)

    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.model(x)
        x= self.dropout(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create an instance of myNet
model = myNet()
print(sum(p.numel() for p in model.parameters()))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

filename = 'checkpointsoverfit\savebestmodel.pt'

checkpoint = torch.load(filename)

model.load_state_dict(checkpoint['state_dict'])

model.to(device)


summary(model,(3,224,224))

print('Number of trainiable prameters: ',sum(p.numel() for p in model.parameters()))


optimizer = optim.Adam(model.parameters(), lr=0.001)
# optimizer = optim.SGD(model.parameters(), lr=1e-03, momentum=0.9)
criterion = nn.CrossEntropyLoss()
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)


def save_ckp(state, is_best, checkpoint_path, best_model_path):

    f_path = checkpoint_path
    torch.save(state, f_path)
    if is_best:
        best_fpath = best_model_path
        shutil.copyfile(f_path, best_fpath)


# Train the model


num_epochs = 10
for epoch in range(num_epochs):
    print(epoch)
    running_loss = 0.0
    running_corrects = 0

    running_train_loss = 0.0
    running_train_accuracy = 0.0

    # model.train()
    # for i, data in enumerate(train_loader, 0):
    #     inputs, labels = data
    #     if device == 'cuda':
    #         inputs,lables = inputs.cuda(),labels.cuda()
    #     inputs, labels = inputs.to(device), labels.to(device)
    #     optimizer.zero_grad()
    #     outputs = model(inputs)
    #     loss = criterion(outputs, labels)
    #     loss.backward()
    #     optimizer.step()
    #     _, predicted = outputs.max(1)
    #     correct = predicted.eq(labels).sum().item()
    #     accuracy = correct / labels.size(0)
    #     running_train_loss += loss.item()
    #     running_train_accuracy += accuracy
    
    # train_loss = running_train_loss / len(train_loader)
    # train_accuracy = running_train_accuracy / len(train_loader)
    # writer.add_scalar("Loss/train", train_loss, epoch)
    # writer.add_scalar("accuracy/train", train_accuracy, epoch)
    # print(' Epoch %d Training loss: %.3f acc: %.3f' % (epoch + 1, train_loss, train_accuracy))

    correct = 0
    total = 0
    model.eval()
    model.to(device)
    running_val_loss = 0.0
    running_val_accuracy = 0.0

    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            if device == 'cuda':
                inputs,lables = inputs.cuda(),labels.cuda()
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, predicted = outputs.max(1)
            correct = predicted.eq(labels).sum().item()
            accuracy = correct / labels.size(0)
            running_val_loss += loss.item()
            running_val_accuracy += accuracy
    
    val_loss = running_val_loss / len(test_loader)
    val_accuracy = running_val_accuracy / len(test_loader)
    writer.add_scalar("Loss/test", val_loss, epoch)
    writer.add_scalar("accuracy/test", val_accuracy, epoch)
    print(f'Epoch [{epoch+1}/{num_epochs}] Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}')

    checkpoint = {
            'epoch': epoch + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
    
    # save_ckp(checkpoint, False, checkpoint_path, best_model_path)
    
    # if val_loss <= valid_loss_min:
    #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
    #     save_ckp(checkpoint, True, checkpoint_path, best_model_path)
    #     valid_loss_min = val_loss
    
    # scheduler.step()
    # print("Learning rate:", scheduler.get_last_lr())



writer.flush()
print('Finished Testing')
writer.close